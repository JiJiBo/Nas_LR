## **PUCT 公式**（AlphaZero 用的版本）

$$
a^* = \arg\max_a \; Q(s,a) + U(s,a)
$$

其中：

$$
U(s,a) = c_{\text{puct}} \cdot P(s,a) \cdot \frac{\sqrt{\sum_b N(s,b)}}{1 + N(s,a)}
$$

---

### **符号说明**

| 符号                | 含义                                     |
|-------------------|----------------------------------------|
| $s$               | 当前状态（棋盘局面）                             |
| $a$               | 某个可能的动作（落子位置）                          |
| $Q(s,a)$          | **平均价值**：从状态 s 走 a 后的平均胜率（累计价值 / 访问次数） |
| $N(s,a)$          | 从 s 走 a 的访问次数（visit count）             |
| $\sum_b N(s,b)$   | 当前状态 s 所有动作的访问次数总和                     |
| $P(s,a)$          | **先验概率（prior）**，由策略网络预测该动作的好坏          |
| $c_{\text{puct}}$ | 探索系数，控制探索项 U 的权重（越大越爱探索）               |

---

### **两部分的作用**

* **利用（Exploitation）**：
  $Q(s,a)$ → 选历史上表现最好的动作。
* **探索（Exploration）**：
  $U(s,a)$ → 优先去探索网络认为（P 大）但访问次数少（N 小）的动作。

---

### **直觉例子**

假设：

* Q = 0.5 （历史胜率 50%）
* P = 0.2 （网络觉得概率 20%）
* N(s,a) = 2 （走过 2 次）
* 总访问数 $\sum N = 100$
* c\_puct = 1.5

那么：

$$
U = 1.5 \cdot 0.2 \cdot \frac{\sqrt{100}}{1 + 2}
= 0.3 \cdot \frac{10}{3}
\approx 1.0
$$

选择分数：

$$
Q + U = 0.5 + 1.0 = 1.5
$$

这个分数和其他动作比，决定了下一步模拟走哪边。
 