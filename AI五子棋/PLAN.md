# 总览

* **目标**：从零实现可自博弈训练的 AlphaZero 式五子棋 AI：MCTS（PUCT）+ 策略价值网络 + 自对弈数据迭代。
* **时间**：6 周（你工作日晚 7–10 点 + 周日整天），可边做边跑实验。
* **里程碑**：

    1. 可靠的棋盘与裁判 → 2) 纯 MCTS baseline → 3) 可用的策略价值网 → 4) 端到端自对弈训练管线 → 5)
       稳定收敛与对局胜率提升 → 6) 对外评测与优化。

---

# 第 0 周（半天即可）：需求与骨架

**产出**：代码仓库骨架、任务清单、可运行的空管线
**任务**

* 选定规则：自由连五 or 连珠禁手？（影响胜负判断与数据输入）
* 仓库结构

  ```
  gomoku/
    core/      # 棋盘、落子、胜负判断、合法走子
    mcts/      # 节点、PUCT、并行搜索
    net/       # 模型、训练、数据管线
    selfplay/  # 自对弈生成器
    arena/     # 基准对战与Elo
    utils/
  ```
* 设定统一随机种子、日志方案（tensorboard/CSV）、配置系统（yaml）。

---

# 第 1 周：棋盘与裁判 + 单元测试

**产出**：确定无误的规则引擎与状态表示
**任务**

* 棋盘：15×15，状态包含：当前棋子面、对手棋子面、己方执子、上一步落子坐标。
* 接口：`reset() / step(move) / legal_moves() / is_terminal() / winner() / render()`
* **胜负判断**：高效连五检测（横/竖/两斜）；和棋（棋满）处理。
* **单测**：覆盖边界（四连/六连/双活三等）与性能（每秒状态评估次数）。
  **验收**：1e6 次随机落子+回滚不崩溃；关键场景断言全部通过。

---

# 第 2 周：纯 MCTS（PUCT）Baseline

**产出**：无神经网的 MCTS 玩家（先随即先验 or 轻度启发）
**任务**

* 节点结构：`N(s,a), W(s,a), Q(s,a), P(s,a)`；子表：move -> (child, prior)。
* 公式（PUCT）：
  `U = c_puct * P * sqrt(ΣN) / (1 + N_child)`；选择 `argmax(Q + U)`
* 扩展与回传：终局奖励 ∈ {+1, -1, 0}（从当前玩家视角）。
* 实参：`c_puct=1.5~2.5`，每步模拟数 `sims=800`（先 200 起步调通）。
* **并发**：先单线程；预留虚拟损失（virtual loss）接口以便后续多线程。
  **验收**：自战胜随机或简单启发对手 ≥80%；每步 1s 内完成 200 次模拟（视机器）。

---

# 第 3 周：策略-价值网络 MVP

**产出**：能前向推理的网络与训练脚本（先小模型）
**模型**

* **输入平面（建议 10\~12 个）**：我方落子、对方落子、上一步坐标（one-hot 两平面）、当前执子、（可选）禁手标记、空位/合法位掩码等。
* **架构**：轻量 ResNet

    * Stem：3×3 conv 128 通道
    * 6–10 个残差块（3×3）
    * Head：

        * Policy head：1×1 conv → flatten → 全连接到 225 维（15×15），对非法着法 masked softmax
        * Value head：1×1 conv → 全连接 → tanh 标量
* **损失**：`L = CE(policy, π_MCTS) + MSE(value, z) + λ||θ||²`（λ≈1e-4）
* **优化**：AdamW，lr=1e-3，cosine 或 step；batch=256（按显存调）。
* **数据增强**：8 向对称（旋转/翻转）。
  **验收**：在随机自博弈数据上能稳定拟合；前向 < 2ms/步（按设备）。

---

# 第 4 周：AlphaZero 自对弈闭环

**产出**：端到端自对弈→训练→评估流水线
**自对弈设置**

* MCTS 使用网络给的 `(P, V)`：扩展时用 `P` 作为 prior，回传用终局 `z∈{-1,0,1}`。
* 根节点注入 Dirichlet 噪声：`P' = (1-ε)P + εDir(α)`；ε=0.25，α≈0.3（可按合法着法数缩放）。
* 温度 `τ`：前 10 手用 `τ=1` 提高探索，之后 `τ→0` 取访问数最多的着法。
* 认输门槛：若若干步滚动平均价值 < -0.8 则可提前认输（减少无效对局）。
* 回放池（replay buffer）：最近 500k～1M 步（样本为〈s, π, z〉）。
  **训练循环**

1. 生成自对弈局面（并行 worker）
2. 训练若干步（从回放池采样）
3. 新模型与旧模型 **Arena** 评估：采用 200–400 局对战，通过率 >55% 才替换。
   **验收**：能稳定滚动产出新 checkpoint，Elo 对 baseline 上升。

---

# 第 5 周：稳定性与速度优化

**产出**：更快的搜索，更稳的训练
**优化点**

* **并行 MCTS**：多进程/线程 + 虚拟损失；批量网络前向（收集若干叶子一起推理）。
* **缓存**：置换表 / 哈希（Zobrist）缓存 Q、N、P；重复局面复用。
* **超参扫描**：`c_puct∈[1,3]`、`sims∈[200,1600]`、ε/α、温度步数、认输阈值。
* **数据均衡**：控制先手/后手比例，首手固定对称开局以提升泛化。
  **验收**：同等时延下胜率提升；自对弈吞吐↑，训练 loss 稳定。

---

# 第 6 周：评测、可视化、对外接口

**产出**：可与人类对局的 App/CLI、谱面可视化、报告
**任务**

* **评测**：与纯 MCTS、传统启发式（活三/冲四）进行 1000 局对战（换先）。
* **指标**：胜率、平均步时、Elo；开局前 20 步分布覆盖度。
* **可视化**：落子热力图、MCTS 访问数分布、价值曲线。
* **API/集成**：提供 `think(board, time_ms)` 接口；移动端/网页前端对接。
  **验收**：对 baseline 胜率≥70%，对启发式≥60%（按你的算力与时限调）。

---

# 关键实现细节（易踩坑清单）

* **价值标注**：`z` 必须从**当前玩家视角**一致回传，否则会震荡。
* **非法着法 mask**：policy head 对非法位置零并重归一；训练时也要 mask。
* **温度与探索**：前期不够探索会陷入局部套路；保持 τ 与 Dirichlet 噪声。
* **对称增强**：自博弈样本必做 8 向增强，且 policy π 同步变换。
* **复盘与断点**：自对弈与训练都要可恢复；写入版本号与配置快照。
* **禁手**（如需）：影响胜负判断与合法着法，另外建议加“禁手面”特征。

---

# 初始超参建议（起跑线）

* `board=15x15`；`sims/train=400`、`sims/arena=800`
* `c_puct=2.0`；根噪声 `ε=0.25, α=0.3`
* `τ=1@前10手，之后=0`；`resign_threshold=-0.8（连续6步）`
* 训练：`batch=256`，`lr=1e-3 cosine`，`weight_decay=1e-4`，`epochs_per_iter=1~2`
* 回放池：`≥500k` 样本；每次训练采样时按最近/较旧 7:3 混合

---

# 时间表（结合你晚 7–10 点 + 周日）

* **周一到周六晚**：实现/修 bug/小规模实验（3h）
* **周日**：跑长测与 Arena、大回归剖析（8–10h）
* 建议每周末固定输出：**一页周报**（胜率曲线、损失、关键盘分析与改动清单）。

---

# 目录与接口清单（落地方便）

* `core/board.py`：位棋盘（bitboard）实现、Zobrist、胜负检测
* `mcts/tree.py`：Node/Edge、select/expand/backup、并行队列
* `mcts/search.py`：`mcts_search(state, net, sims, time_ms=None)`
* `net/model.py`：ResNet；`forward -> (logits, value)`
* `net/train.py`：数据集〈s, π, z〉、增强、训练循环
* `selfplay/worker.py`：生成对弈、落盘、写回放池（LMDB/Parquet）
* `arena/arena.py`：新旧模型对战、统计 Elo
* `tools/visual.py`：热力图、访问数、价值曲线

---